{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cg047ujRBmtU"
   },
   "source": [
    "# Decision Trees Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLXpoHg64HlD"
   },
   "source": [
    "## Implementing Random Forest From Scratch\n",
    "In this exercise you will need to implement a simple version of Random Forest Regressor from scratch. Your decision tree will handle **continuous input and output** (this should actually work also for binary input attributes). \n",
    "\n",
    "* Compelete the skeleton class below (hint: you should also create a `DecisionTree` class that the `TreeEnsemble` will use)\n",
    "  - `X` is a matrix of data values (rows are samples, columns are attributes)\n",
    "  - `y` is a vector of corresponding target values\n",
    "  - `n_trees` is the number of trees to create\n",
    "  - `sample_sz` is the size of the sample set to use of each of the trees in the forest (chose the samples randomly, with or without repetition)\n",
    "  - `min_leaf` is the minimal number of samples in each leaf node of each tree in the forest\n",
    "  \n",
    "* For splitting criterion, use either **\"Train Squared Error Minimization (Reduction in Variance)\"** or **\"Train Absolute Error Minimization\"** (choose one). Whatever you choose, make sure you implement the splitting point decision efficiently (in $O(n)$ time).\n",
    "\n",
    "* The `predict` function will use mean of the target values in the leaf node matching each row of the given `X`. The result is a vector of predictions matching the number of rows in `X`.\n",
    "\n",
    "* The `oob_mse` function will compute the mean squared error over all **out of bag (oob)** samples. That is, for each sample calculate the squared error using  predictions from the trees that do not contain x in their respective bootstrap sample, then average this score for all samples. See:  [OOB Errors for Random Forests](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html).\n",
    "\n",
    "* To check your random forest implementation, use the bostom dataset (`from sklearn.datasets import load_boston`)\n",
    "\n",
    "  - Use the following to estimate what are the best hyper parameters to use for your model\n",
    "```\n",
    "for n in [1,5,10,20,50,100]:\n",
    "  for sz in [50,100,300,500]:\n",
    "    for min_leaf in [1,5]:\n",
    "      forest = TreeEnsemble(X, y, n, sz, min_leaf)\n",
    "      mse = forest.oob_mse()\n",
    "      print(\"n_trees:{0}, sz:{1}, min_leaf:{2} --- oob mse: {3}\".format(n, sz, min_leaf, mse))\n",
    "```\n",
    "  \n",
    "  - Using your chosen hyperparameters as a final model, plot the predictions vs. true values of all the samples in the training set . Use something like:\n",
    "  ```\n",
    "  y_hat = forest.predict(X)  # forest is the chosen model\n",
    "  plt.scatter(y_hat, y)\n",
    "  ```\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA54r4DiQDkM"
   },
   "outputs": [],
   "source": [
    "class TreeEnsemble():\n",
    "    def __init__(self, X, y, n_trees, sample_sz, min_leaf):\n",
    "        self.trees = []\n",
    "        self.tree_to_same_indexes = []\n",
    "        self.sample_data_X = X\n",
    "        self.sample_data_y = y\n",
    "\n",
    "        for tree_index in range(n_trees):\n",
    "            samle_data_indexes = np.random.choice(range(X.shape[0]), sample_sz)\n",
    "\n",
    "            sample_data_x = X[samle_data_indexes]\n",
    "            sample_data_y = y[samle_data_indexes]\n",
    "            dt = DecisionTree(sample_data_x, sample_data_y, min_leaf)\n",
    "\n",
    "            self.tree_to_same_indexes.append(samle_data_indexes)\n",
    "            self.trees.append(dt)\n",
    "\n",
    "    def predict(self, X):\n",
    "         return TreeEnsemble.all_trees_predist(self.trees, X)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def all_trees_predist(trees, X):\n",
    "        indexes = range(len(X))\n",
    "        X = np.column_stack((X, indexes))\n",
    "\n",
    "        all_trees_results = None\n",
    "        for tree in trees:\n",
    "            res = tree.predict(X)\n",
    "            results = res[res[:, 0].argsort()]\n",
    "            final_results = results[:, -1]\n",
    "\n",
    "            if all_trees_results is not None:\n",
    "                all_trees_results = np.column_stack((all_trees_results, final_results))\n",
    "            else:\n",
    "                all_trees_results = final_results\n",
    "\n",
    "        if all_trees_results.shape[1] > 1:\n",
    "            return np.average(all_trees_results, axis=1)\n",
    "        else:\n",
    "            return all_trees_results[0]\n",
    "\n",
    "    def oob_mse(self):\n",
    "\n",
    "        error_sum = 0.0\n",
    "        count = 0.0\n",
    "        for index, y in enumerate(self.sample_data_y):\n",
    "            trees = []\n",
    "            for tree_index, sample in  enumerate(self.tree_to_same_indexes):\n",
    "                if index not in sample:\n",
    "                    trees.append(self.trees[tree_index])\n",
    "\n",
    "            if len(trees) > 0:\n",
    "                result = TreeEnsemble.all_trees_predist(trees, X[index: index+1, :])\n",
    "                error_sum += math.pow(result[0] - y, 2)\n",
    "                count += 1\n",
    "\n",
    "        return math.sqrt(error_sum / count)\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, ):\n",
    "        self.vote = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature = None\n",
    "        self.feature_value = None\n",
    "\n",
    "    def condition(self, data):\n",
    "        data_to_left = data[data[:, self.feature] > self.feature_value]\n",
    "        data_to_right = data[data[:, self.feature] <= self.feature_value]\n",
    "\n",
    "        return data_to_left, data_to_right\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, X, y,  min_leaf):\n",
    "\n",
    "        self.root = Node()\n",
    "\n",
    "        currentNode = self.root\n",
    "        data = np.column_stack((X, y))\n",
    "\n",
    "        self.create_tree(data, currentNode, min_leaf)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "\n",
    "        return self._predict(self.root, X)\n",
    "\n",
    "    def _predict(self, node, data):\n",
    "        if node.vote:\n",
    "            #return data[:, data[-1]], node.vote\n",
    "            vals = np.full(shape=(data.shape[0], 1), fill_value=node.vote)\n",
    "            return np.column_stack((data[:, -1], vals))\n",
    "        else:\n",
    "            left_data, right_data = node.condition(data)\n",
    "            left_choices = None\n",
    "            if left_data is not Node and left_data.shape[0] > 0:\n",
    "                left_choices = self._predict(node.left, left_data)\n",
    "\n",
    "            right_choices = None\n",
    "            if right_data is not Node and right_data.shape[0] > 0:\n",
    "                right_choices = self._predict(node.right, right_data)\n",
    "\n",
    "            if left_choices is not None and right_choices is not None:\n",
    "                return np.concatenate((left_choices, right_choices))\n",
    "            elif right_choices is not None:\n",
    "                return right_choices\n",
    "            else:\n",
    "                return left_choices\n",
    "\n",
    "\n",
    "\n",
    "    def create_tree(self, data, parent_node, min_leaf):\n",
    "        best_feature, best_feature_value = self.next_split(data, min_leaf)\n",
    "\n",
    "        if best_feature and best_feature_value:\n",
    "            # left = X[:, X[best_feature] <= best_feature_value]\n",
    "            # right = X[:, X[best_feature] > best_feature_value]\n",
    "\n",
    "            left = data[data[:, best_feature] > best_feature_value]\n",
    "            right = data[data[:, best_feature] <= best_feature_value]\n",
    "\n",
    "            parent_node.feature = best_feature\n",
    "            parent_node.feature_value = best_feature_value\n",
    "\n",
    "            parent_node.left = Node()\n",
    "\n",
    "            if left.shape[0] <= min_leaf:\n",
    "                parent_node.left.vote = np.average(left[:, -1])\n",
    "            else:\n",
    "                self.create_tree(left, parent_node.left, min_leaf)\n",
    "\n",
    "            parent_node.right = Node()\n",
    "\n",
    "            if right.shape[0] <= min_leaf:\n",
    "                parent_node.right.vote = np.average(right[:, -1])\n",
    "            else:\n",
    "                self.create_tree(right, parent_node.right, min_leaf)\n",
    "        else:\n",
    "            parent_node.vote = np.average(data[:, -1])\n",
    "\n",
    "    def next_split(self, data, min_leaf):\n",
    "        num_of_features = data.shape[1] - 1\n",
    "        lowest_score = float('inf')\n",
    "        best_feature = None\n",
    "        best_feature_value = None\n",
    "        for feature in range(num_of_features):\n",
    "            split_options = np.unique(data[:, feature])\n",
    "\n",
    "            for split_option_val in split_options:\n",
    "                left = data[data[:, feature] > split_option_val]\n",
    "                left_variance = np.std(left[:, left.shape[1] - 1])\n",
    "\n",
    "                right = data[data[:, feature] <= split_option_val]\n",
    "\n",
    "                right_variance = np.std(right[:, right.shape[1] - 1])\n",
    "\n",
    "                score = \\\n",
    "                        (left_variance * (left.shape[0] / data.shape[0])) \\\n",
    "                        + (right_variance * (right.shape[0] / data.shape[0]))\n",
    "\n",
    "                if score < lowest_score:\n",
    "                    lowest_score = score\n",
    "                    best_feature = feature\n",
    "                    best_feature_value = split_option_val\n",
    "\n",
    "\n",
    "        return best_feature, best_feature_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.01266592341679"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np \n",
    "\n",
    "boston_data = load_boston()\n",
    "X = boston_data['data']\n",
    "Y = boston_data['target']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=999)\n",
    "ensemble = TreeEnsemble(X_train, Y_train, 100, 250, 4)\n",
    "results = ensemble.predict(X_test)\n",
    "\n",
    "np.sum(np.square((results - Y_test))) / len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TF5TjNuvTKof"
   },
   "source": [
    "## Using Decision Tree and Random Forest for Digits Classification\n",
    "Remeber the MNIST dataset used - you will now test the power of decision trees on this problem.\n",
    "This time you are given a free hand in choosing the test and train set sizes, model parameters (such as gain function and constraints over the trees) and features (whether to use binary pixel values or the original continous gray value).\n",
    "- Choose which model parameters you wish to optimize, explain how would you do that, and find a model which you believe would have the minimal generalization error --- do this for both a single decision tree model, and a random forest.\n",
    "  - You can use `sklearn.tree.DecisionTreeClassifier` and `sklearn.ensemble.RandomForestClassifier`\n",
    "- Once you are satisfied with the model parameters, plot for each of the models (a single decision tree and random forest) the importance of each of the pixels to the final decision.\n",
    "- Last, estimate the class assignment probabilities for all the correctly classified and misclassified examples in your test data.\n",
    "- Discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-k9WpIV_n7Y"
   },
   "outputs": [],
   "source": [
    "# code and answer go here\n",
    "import  sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "mnist = sklearn.datasets.fetch_mldata('MNIST original',data_home='/home/gabib3b/code/yandex/intotoml/logisticregression/.ipynb_checkpoints/mnist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "a_train, a_test, b_train, b_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87214285714285711"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=4, min_samples_split=6)\n",
    "clf.fit(a_train, b_train)\n",
    "res = clf.predict(a_test)\n",
    "np.sum(res ==b_test)/(len(res) *1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=200, min_samples_leaf=6)\n",
    "clf.fit(a_train, b_train)\n",
    "res = clf.predict(a_test)\n",
    "np.sum(res ==b_test)/(len(res) *1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GSFGrtOF4B"
   },
   "source": [
    "## References\n",
    "- https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Decision Trees - Exercise.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
